{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from add_parent_to_sys_path import add_parent_to_sys_path\n",
    "add_parent_to_sys_path()\n",
    "from modules.sftp_ops import *\n",
    "import json \n",
    "import logging\n",
    "clear_logging_handlers()\n",
    "\n",
    "#Configure loggging\n",
    "logging.basicConfig(filename='../logs/SFTP_operations.log', level=logging.INFO,\n",
    "                   format='%(asctime)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S')\n",
    "logging.info('\\n\\n-------------New SFTP Operations Logging Instance')\n",
    "\n",
    "\n",
    "with open('../powerschool-420113-db919282054b.json') as json_file:\n",
    "    j = json.load(json_file) \n",
    "    savva_password = j['savva_password']\n",
    "    savva_username = j['savva_username']\n",
    "    savva_host = j['savva_host']\n",
    "    clever_export_password = j['clever_export_password']\n",
    "    clever_export_username = j['clever_export_username']\n",
    "    clever_export_host = j['clever_export_host']\n",
    "\n",
    "\n",
    "\n",
    "#All BQ tables that are begin queryed are the keys. \n",
    "#How they are saved in the local dir are the values\n",
    "clever_dictionary = {'Clever_schools':'schools.csv',\n",
    "                    'Clever_students':'students.csv',\n",
    "                    'Clever_teachers':'teachers.csv',\n",
    "                    'Clever_sections':'sections.csv',\n",
    "                    'Clever_enrollments':'enrollments.csv', \n",
    "                    'Clever_staff': 'staff.csv'\n",
    "                    }\n",
    "\n",
    "savva_dictionary = {\n",
    "                    'SAVVAS_CODE_DISTRICT': 'CODE_DISTRICT.txt',\n",
    "                    'SAVVAS_SCHOOL': 'SCHOOL.txt',\n",
    "                    'SAVVAS_STUDENT': 'STUDENT.txt',\n",
    "                    'SAVVAS_STAFF': 'STAFF.txt',\n",
    "                    'SAVVAS_PIF_SECTION': 'PIF_SECTION.txt',\n",
    "                    'SAVVAS_PIF_SECTION_STAFF': 'PIF_SECTION_STAFF.txt',\n",
    "                    'SAVVAS_PIF_SECTION_STUDENT': 'PIF_SECTION_STUDENT.txt',\n",
    "                    'SAVVAS_ENROLLMENT': 'ENROLLMENT.txt',\n",
    "                    'SAVVAS_ASSIGNMENT': 'ASSIGNMENT.txt'\n",
    "                }\n",
    "\n",
    "\n",
    "\n",
    "sftp_conn_clever_export = SFTPConnection(\n",
    "    host=clever_export_host,\n",
    "    username=clever_export_username,\n",
    "    password=clever_export_password,\n",
    "    use_pool=False\n",
    ")\n",
    "\n",
    "#Export BQ views to local dir. Follow naming convention of dictionary arg. \n",
    "SFTP_conn_file_exchange(sftp_conn_clever_export,\n",
    "                        import_or_export = 'export',\n",
    "                        sftp_folder_name='clever_iota_file_transfer', \n",
    "                        db='roster_files',\n",
    "                        naming_dict = clever_dictionary,\n",
    "                        use_pool=False,\n",
    "                        )\n",
    "\n",
    "# Export the local replicated files to Clevers SFTP\n",
    "SFTP_export_dir_to_SFTP(local_dir=os.getcwd() + '\\\\clever_iota_file_transfer',\n",
    "               remote_dir='/home/boundless-calendar-0789',  #root dir on clevers sftp\n",
    "               sftp = sftp_conn_clever_export)\n",
    "\n",
    "\n",
    "# ----------------------------SAVVA piece-----------------------------------\n",
    "\n",
    "sftp_conn_savva = SFTPConnection(\n",
    "    host=savva_host,\n",
    "    username=savva_username,\n",
    "    password=savva_password,\n",
    "    use_pool=False\n",
    ")\n",
    "\n",
    "\n",
    "#Export replicates BQ views to local dir. Follow naming convention as dictionary arg. \n",
    "SFTP_conn_file_exchange(sftp_conn_savva,\n",
    "                        import_or_export = 'export',\n",
    "                        sftp_folder_name='savva_iota_file_transfer', \n",
    "                        db='roster_files',\n",
    "                        naming_dict = savva_dictionary,\n",
    "                        use_pool=False\n",
    "                        )\n",
    "\n",
    "#Sends local files over the SAVVAS sftp SIS folder\n",
    "SFTP_export_dir_to_SFTP(local_dir=os.getcwd() + '\\\\savva_iota_file_transfer',\n",
    "               remote_dir='/SIS',  #root dir on clevers sftp\n",
    "               sftp = sftp_conn_savva)\n",
    "\n",
    "\n",
    "\n",
    "sftp_conn_clever_export.close_all_connections()\n",
    "sftp_conn_savva.close_all_connections()\n",
    "logging.info('Process has reached the end')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Current run diagnosis. \n",
    "#Remove piplock until ran in virtual env\n",
    "#Implement a catch if the file does not exist in BQ, example is clever_enrollments\n",
    "#Specify which sftp conn is established in logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hi Sam,\n",
    "\n",
    "#Current SFTP operations file running at 5AM. \n",
    "\n",
    "\n",
    "# I'm working on some improvements to the timing and organization of our data processes.\n",
    "# Can you make the changes below?\n",
    "# For the file we are pulling from reports-sftp.clever.com:  idm-sensitive-exports\n",
    "# Clever updates this each morning at 6:45 AM, so pull from their SFTP at 6:50 AM and send to GCS/BQ. \n",
    "# To keep it separate from the exports coming in from PS, let's put this in a separate dataset \n",
    "# (it can be called misc_imports).\n",
    "\n",
    "# Also move the TN_options_report_2014 file form EasyIEP to misc_imports. \n",
    "# It can also be pulled at 6:50 if it's easiest to do those two together.\n",
    "# I have a query set up that creates a list of the credentials that need\n",
    "#  to be updated in PS - it drops those into\n",
    "#  two tables in the powerschool_staged db: GoogleForPSImport_ASD and GoogleForPSImport_TPCSC. \n",
    "# Could we send these back to the IOTA SFTP at 7:05 as two separate .txt files? \n",
    "# Then I'll set up an automated import into each PS so that we have updated credentials before school starts.\n",
    "#  I'll probably also have EasyIEP files to import too at some point in the future.\n",
    "# Let me know if you have any questions or want to talk through!\n",
    "\n",
    "\n",
    "\n",
    "#Seperate Clever import out idm-sensitive-exports make this occur at 6:50 AM label it in a folder called misc imports \n",
    "#Get rid of the older one, bucket, and dataset\n",
    "#This piece is going to need to be updated for Google Cloud send as well. Seperate this piece out. \n",
    "#Same thing needs to occur for TN_options_report_2014. Put it in a misc imports at 6:50 AM. \n",
    "#Tested with the bigqueryoperations\n",
    "\n",
    "#Issue with google-powerschool-students-passwords table creation. \n",
    "#Issue with this email being a column name aaguilon210421@iotaschools_org\n",
    "#See if Amy needs this file, if so need to implement some Dynamic Error catching with optional args\n",
    "\n",
    "\n",
    "#THis will be another seperate script\n",
    "#Pull these Big Query tables down GoogleForPSImport_ASD and GoogleForPSImport_TPCSC send these to IOTA SFTP at 7:05\n",
    "#This will eventually be expanded out. \n",
    "\n",
    "\n",
    "#Fix import based on folder movement\n",
    "#Delete idm_sensitive_exports files out of Powerschool Combined\n",
    "#Logs paths got changed\n",
    "\n",
    "#Test out that nestd file paths are working\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
